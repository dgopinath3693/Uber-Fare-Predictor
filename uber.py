# -*- coding: utf-8 -*-
"""Uber

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNQSF-pJQ8-uOzZ8eintED9gZnQbOxKt
"""

import collections.abc
#hyper needs the four following aliases to be done manually.
collections.Iterable = collections.abc.Iterable
collections.Mapping = collections.abc.Mapping
collections.MutableSet = collections.abc.MutableSet
collections.MutableMapping = collections.abc.MutableMapping
#Now import hyper

!pip install mastml

!pip install scikit-learn==1.1.3

!pip install geopy

import os                        # OS stands for Operating System and provides ways for python to interact with files or directories
from collections import Counter  # Collections is a package for handling data
from pprint import pprint

import pandas as pd              # Pandas is a data analysis library which we'll primarily use to handle our dataset
import numpy as np               # Numpy is a package for scientific computing. We'll use it for some of it's math functions
import pymatgen                  # Pymatgen is a library for materials analysis which we use to interpret our material compositions

import sklearn                   # Scikit-learn is a machine learning package, providing the backbone for the work we'll perform
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_validate,GridSearchCV,ParameterGrid
from sklearn.model_selection import KFold,RepeatedKFold

from mastml.mastml import Mastml
from mastml.datasets import SklearnDatasets
from mastml.preprocessing import SklearnPreprocessor
from mastml.models import SklearnModel
from mastml.data_splitters import SklearnDataSplitter

from mastml.mastml import Mastml
from mastml.datasets import LocalDatasets
from mastml.data_cleaning import DataCleaning
from mastml.preprocessing import SklearnPreprocessor
from mastml.models import SklearnModel
from mastml.data_splitters import SklearnDataSplitter, NoSplit
from mastml.feature_selectors import EnsembleModelFeatureSelector, NoSelect
from mastml.feature_generators import ElementalFeatureGenerator
from mastml.hyper_opt import GridSearch

import matplotlib                # Matplotlib is the plotting package that we'll use throughout the lab
import matplotlib.pyplot as plt
import seaborn as sns            # Seaborn is a Python data visualization library based on matplotlib

import graphviz                  # graphviz is a package that helps visualize decision trees

# Read in the band gap data from our dataset
mastml_df = pd.read_csv("uberRand.csv")

mastml_df

mastml_df_filtered = mastml_df[mastml_df["passenger_count"]==1]
mastml_df_filtered = mastml_df_filtered.dropna()
mastml_df_filtered = mastml_df_filtered.drop(columns = 'Unnamed: 1')

mastml_df_clean = mastml_df_filtered

distance = list()
for index in mastml_df_clean.index:
  startx = mastml_df_clean['pickup_longitude'][index]
  starty = mastml_df_clean['pickup_latitude'][index]
  endx = mastml_df_clean['dropoff_longitude'][index]
  endy = mastml_df_clean['dropoff_latitude'][index]
  distanceTuple = (startx, starty, endx, endy)
  distance.append(distanceTuple)

mastml_df_clean["Condensed Distance"] = distance

print("Hello")

# Getting rid of duplicate values in the condensed distance
mastml_df_clean = mastml_df_clean.drop_duplicates(subset = ["Condensed Distance"])
print(len(pd.unique(mastml_df_clean["Condensed Distance"])))
mastml_df_clean

from math import sqrt

mastml_df_clean

from geopy.distance import geodesic

# pickup = (mastml_df_clean["pickup_latitude"], mastml_df_clean["pickup_longitude"])
# dropoff = (mastml_df_clean["dropoff_latitude"], mastml_df_clean["dropoff_longitude"])
distance = list()
for idx in mastml_df_clean.index:
  pickup = (mastml_df_clean["pickup_latitude"][idx], mastml_df_clean["pickup_longitude"][idx])
  dropoff = (mastml_df_clean["dropoff_latitude"][idx], mastml_df_clean["dropoff_longitude"][idx])
  temp = (geodesic(pickup, dropoff).km)
  distance.append(temp)
mastml_df_clean["distance (KM)"] = distance

# generate basic statistics on our band gap values
mastml_df_clean["fare_amount"].describe().round(3)

# we'll also define a simple histogram plotting function to use later
def histogram_plot(data):
    fig1,ax1 = plt.subplots()
    ax1.hist(data,bins=range(13),density=1)
    ax1.set_xticks(range(13))
    ax1.set_xlabel('FareAmount ($)')
    ax1.set_ylabel('Distance (km)')
    plt.show()

histogram_plot(mastml_df_clean["fare_amount"].astype("float"))

SAVEPATH = 'drive/MyDrive/Uber'

# the Mastml call here is initializing the Mastml object
mastml = Mastml(savepath=SAVEPATH)
# the get_savepath method generates the output folder specified above
savepath = mastml.get_savepath

# Output data to csv - note depending on when you run this the updated data file may have been pregenerated so this cell isn't technically necessary.
output_path = "uberRand.csv"

if os.path.isfile(output_path):
    print(output_path," exists, not creating new file")
else:
    mastml_df_clean.to_csv(output_path)

mastml_df_clean

#split features_df into two dataframes
target_data_df = pd.DataFrame([mastml_df_clean["fare_amount"]])
features_df = mastml_df_clean.drop(columns=['Index','key','pickup_datetime','passenger_count','sort', 'Condensed Distance', 'fare_amount', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])

features_df = features_df.dropna()
features_df

minmax_features = MinMaxScaler().fit_transform(features_df)
minmax_features_df = pd.DataFrame(minmax_features,columns=features_df.columns)
minmax_features_df.iloc[:5, :5]

X = features_df                         # inputs/features
y = pd.DataFrame([mastml_df_clean["fare_amount"]]) # outputs/targets

y = y.T
y

preprocessor = SklearnPreprocessor(
                                    preprocessor='MaxAbsScaler', as_frame=True
                                   )

X = preprocessor.evaluate(X, savepath=savepath)

model1 = SklearnModel(model='LinearRegression')
model2 = SklearnModel(model='KernelRidge', kernel='rbf')

models = [model1,model2]

splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=10)

y

splitter.evaluate(X=X,
                  y=y,
                  models=models,
                  preprocessor=preprocessor,
                  savepath=savepath,
                  verbosity=3)

model3 = SklearnModel(model='RandomForestRegressor')

splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=1, n_splits=5)
splitter.evaluate(X=X,
                  y=y,
                  models=[model3],
                  preprocessor=preprocessor,
                  savepath=savepath,
                  verbosity=3)

default_decisiontree = SklearnModel(model='DecisionTreeRegressor')
models = [default_decisiontree]
selector = [NoSelect()]
metrics = ['r2_score']

splitter = NoSplit()
splitter.evaluate(X=X,
                  y=y,
                  models=models,
                  preprocessor=None,
                  selectors=selector,
                  metrics=metrics,
                  savepath=savepath,
                  # leaveout_inds=X_testdata,
                  verbosity=3)

default_RF = SklearnModel(model='RandomForestRegressor')
models = [default_RF]
selector = [NoSelect()]
metrics = ['r2_score']
grid1 = GridSearch(param_names='n_estimators',param_values='2 50 5 lin int',scoring='root_mean_squared_error')
grids = [grid1]
splitter = NoSplit()
splitter.evaluate(X=X,
                  y=y,
                  models=models,
                  preprocessor=None,
                  selectors=selector,
                  metrics=metrics,
                  savepath=savepath,
                  hyperopts = grids,
                  recalibrate_errors = True,
                  verbosity=3)

default_decisiontree = SklearnModel(model='DecisionTreeRegressor')
models = [default_decisiontree]
selector = [NoSelect()]
metrics = ['r2_score']

splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=2, n_splits=5)
splitter.evaluate(X=X,
                  y=y,
                  models=models,
                  preprocessor=None,
                  selectors=selector,
                  metrics=metrics,
                  savepath=savepath,
                  verbosity=3)

opt_RF = SklearnModel(model='RandomForestRegressor',n_estimators=50)
models = [opt_RF]
selector = [NoSelect()]
metrics = ['r2_score']

splitter = SklearnDataSplitter(splitter='RepeatedKFold', n_repeats=2, n_splits=5)
splitter.evaluate(X=X,
                  y=y,
                  models=models,
                  preprocessor=None,
                  selectors=selector,
                  metrics=metrics,
                  savepath=savepath,
                  recalibrate_errors = True,
                  verbosity=3)